"""
Algorithm to compute required articles applying the pagerank algorithm to a graph generated by 
tranversing the target article by a few levels.
"""

import sys

import py2neo
from py2neo import Graph

import networkx as nx

import matplotlib.pyplot as plt

print "Initing db..."

#Load database
py2neo.authenticate("localhost:7474", "neo4j", "lucas")
graph = Graph("http://localhost:7474/db/data/")

articleTitle = sys.argv[1]

if not sys.argv[2]:
    transversalLevel = "1..2"
else:
    transversalLevel = sys.argv[2]

#Create a directed graph
G = nx.DiGraph()

#Only forward query
# #Construct the query
# dbQuery = " ".join([
#     'MATCH (n1:Article {title:"ARTICLE-TITLE"})-[l1:ConnectsTo*1..1]->(n2:Article)',
#     'RETURN l1'
# ]).replace("ARTICLE-TITLE", articleTitle)


#All directions query based on forward nodes
#Query all nodes id related
dbQuery = " ".join([
    'MATCH (n1:Article {title:"ARTICLE-TITLE"})-[l1:ConnectsTo*' + transversalLevel + ']->(n2:Article)',
    'RETURN collect(DISTINCT ID(n1)) + collect(DISTINCT ID(n2)) as ids'
]).replace("ARTICLE-TITLE", articleTitle)

nodeIds = [] #Array to keep node ids

print "Querying ids..."

#Execute query and compute ids
for r in graph.run(dbQuery):
    nodeIds += r['ids']

#Now query all connectTo relations between these nodes
dbQuery = " ".join([
    'MATCH (n1:Article)-[l1:ConnectsTo]-(n2:Article)',
    'WHERE (ID(n1) IN NODE-IDS AND ID(n2) IN NODE-IDS)',
    'RETURN l1 as edges'
]).replace("NODE-IDS", str(nodeIds))

print "Querying edges..."

#Execute query and add edges
for val in graph.run(dbQuery):
    e = val[0]
    G.add_edge(e.start_node()['title'], e.end_node()['title'])

print "" #Skip line

connection = "strong"
method = "pagerank"

#Get the connected nodes of G according to choose method
if connection == 'weak':
    sc_nodes = nx.weakly_connected_components(G)
elif connection == 'strong':
    sc_nodes = nx.strongly_connected_components(G)

subgraphs = [G.subgraph(nlist) for nlist in sc_nodes]

#Function to compute incomming edges
def sumInEdges(g):
    connDict = dict() #Dictionary to compute values
    edges = g.edges() #get graph edges

    if not edges: #If there is no edges
        connDict[g.nodes()[0]] = 0 #Put only the first node found and return

    for e in edges: #iterate thru edges
        target = e[1] #get target
        if not target in connDict: #if the target has not been initiated @ the dict,
            connDict[target] = 0 #init it
        connDict[target] += 1 #Sum one value
    
    return connDict #return the dict

#Array to keep the clusters found 
clusters = []

#Iterate thru the subgraphs
for dir_sg in subgraphs:
    #Compute the most import node of the cluster
    #We may use the page rank, or only the incomming edges sum

    if method == 'pagerank':
        pr = nx.pagerank(dir_sg).items() #Pagerank
    elif method == 'sum':
        pr = sumInEdges(dir_sg).items() #Incomming edges

    biggestPr = max(pr, key=lambda a: a[1])[0] #Select the biggest value to represent the cluster
    clusters.append((biggestPr, len(dir_sg.nodes()))) #Append it to the subgraphs with the number of nodes

#Print sorted results
for n in sorted(clusters, key=lambda a: a[1], reverse=True):
    print n


sys.exit()

#Execute query 
#iterate over the results
#And add edges
# for r in graph.run(dbQuery):
#     for e in r[0]:
#         G.add_edge(e.start_node()['title'], e.end_node()['title'])
#         # print (e.start_node()['title'], e.end_node()['title'])

print len(G.edges())

resultLog = "\n\r".join(map(str, sorted(nx.pagerank(G).items(), key=lambda a: a[1], reverse=True)))

#save resultLog
with open("results/" + articleTitle + " - " + transversalLevel + ".txt", "w") as f:
    f.write(resultLog)
f.close()

print resultLog 

# for comp in nx.weakly_connected_components(G):
#     print comp

# for e in G.edges():
#     print e


# nx.draw(G)
# plt.show()